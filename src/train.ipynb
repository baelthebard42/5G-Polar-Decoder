{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "589f41f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anjal/Desktop/Code/mamba-seq-pred/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataset import PolarDecDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from models.wrappers.mamba_32bits import MambaPolarDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d85178a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 32\n",
    "CONFIG_NO = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e6fb613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78f1e95",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36e1bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PolarDecDataset(snr_db=10, num_samples=1000000)\n",
    "test_set = PolarDecDataset(snr_db=10, num_samples=1000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdad0b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset, batch_size = 32)\n",
    "test_dataloader = DataLoader(test_set, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbbd230",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17a2a925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaPolarDecoder(\n",
       "  (discrete_embedding): Embedding(2, 64)\n",
       "  (linear_embedding1): Linear(in_features=1, out_features=64, bias=True)\n",
       "  (linear_embedding2): Linear(in_features=1, out_features=64, bias=True)\n",
       "  (linear_input_layer): Linear(in_features=192, out_features=64, bias=True)\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0): BiMambaEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x BiMambaBlock(\n",
       "          (pre_ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (mamba_f): Mamba(\n",
       "            (in_proj): Linear(in_features=64, out_features=256, bias=False)\n",
       "            (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)\n",
       "            (act): SiLU()\n",
       "            (x_proj): Linear(in_features=128, out_features=68, bias=False)\n",
       "            (dt_proj): Linear(in_features=4, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=64, bias=False)\n",
       "          )\n",
       "          (post_ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (ffn_f): Sequential(\n",
       "            (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (pre_ln_r): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (post_ln_r): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (mamba_r): Mamba(\n",
       "            (in_proj): Linear(in_features=64, out_features=256, bias=False)\n",
       "            (conv1d): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)\n",
       "            (act): SiLU()\n",
       "            (x_proj): Linear(in_features=128, out_features=68, bias=False)\n",
       "            (dt_proj): Linear(in_features=4, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=64, bias=False)\n",
       "          )\n",
       "          (ffn_r): Sequential(\n",
       "            (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_norms): ModuleList(\n",
       "    (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (final_proj_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MambaPolarDecoder(\n",
    "    d_model=64,\n",
    "    num_layer_encoder=1,\n",
    "    num_layers_bimamba_block=32,\n",
    "    seq_len=N,\n",
    "    d_state=32,\n",
    "    d_conv=4,\n",
    "    expand=2\n",
    ").to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f20a282",
   "metadata": {},
   "source": [
    "## Minor modification to the Loss Function: Calculates loss only at non frozen positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05e72cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(frozen_bit_prior, target_vector, predicted_vector,  reliable_only=False):\n",
    "    \"\"\"\n",
    "    frozen_bit_prior: tensor of shape (seq_len,) with 1 for frozen, 0 for message bits\n",
    "    target_vector: tensor of shape (seq_len,)\n",
    "    predicted_vector: tensor of shape (seq_len,)\n",
    "    loss_fn: PyTorch loss function\n",
    "    \"\"\"\n",
    "\n",
    "    if reliable_only: \n",
    "     mask = (frozen_bit_prior != 1) \n",
    "     target_vector = target_vector[mask]\n",
    "     predicted_vector = predicted_vector[mask]\n",
    "\n",
    "    #print(\"target vector:\" ,target_vector[:32], \"\\n\")\n",
    "    #print(\"pred vector:\" ,predicted_vector[:32])\n",
    "\n",
    "   # print(f\"Length of reliable bits: {len(reliable_target)}\")\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    return loss_fn(predicted_vector, target_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808bd9a5",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5dc9380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted bits:00110100100001000000000000000000\n",
      "\n",
      "Actual bits: 00011100100000000000000000000000\n",
      "\n",
      "Loss: 0.44349950551986694\n"
     ]
    }
   ],
   "source": [
    "llr, frozen_tensor, snr_tensor, target_tensor= next(iter(train_dataloader))\n",
    "ip1 = llr.float().to(device)\n",
    "ip2 = frozen_tensor.int().to(device)\n",
    "ip3 = snr_tensor.float().to(device)\n",
    "\n",
    "predicted = model(ip1, ip2, ip3) #works\n",
    "\n",
    "loss = calculate_loss(ip2, target_tensor.to(device), predicted.to(device)) #works\n",
    "\n",
    "#print(f\"Channel Observation Vector: {channel_tensor}\\n\\n\")\n",
    "\n",
    "\n",
    "#print(f\"Channel Observation Vector: {ip1.shape}\\nFrozen Tensor: {ip2.shape}\\n\")\n",
    "#print(f\"Predicted Channel Input Vector(logits): {predicted.shape}\\n\\n\")\n",
    "\n",
    "#print(f\"Predicted (sigmoid): {torch.sigmoid(predicted)}\\n\\n\")\n",
    "pred = (torch.sigmoid(predicted) > 0.5).long()[0]\n",
    "\n",
    "print(f\"Predicted bits:{''.join(map(str, pred.cpu().tolist()))}\\n\")\n",
    "print(f\"Actual bits: {''.join(str(int(i)) for i in target_tensor[0])}\\n\")\n",
    "print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69534924",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.001)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                        factor=0.5, patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32f8dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index):\n",
    "\n",
    "    running_loss = 0\n",
    "    last_loss = 0\n",
    "\n",
    "\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        channel_tensor, frozen_tensor, snr_tensor, target_tensor = data\n",
    "        ip1 = channel_tensor.float().to(device)\n",
    "        ip2 = frozen_tensor.int().to(device)\n",
    "        ip3 = snr_tensor.float().to(device)\n",
    "        op = target_tensor.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ip1,ip2 ,ip3 ).to(device)\n",
    "\n",
    "    #    B, L, C = outputs.shape\n",
    "    #    output_logits = outputs.view(B*L, C).to(device)\n",
    "    #    target_flattened = shifted.view(B*L).to(device).long()\n",
    "\n",
    "\n",
    "    #    loss = loss_fn(output_logits, target_flattened)\n",
    "        \n",
    "        loss = calculate_loss(ip2, op, outputs)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i%1000 == 999:\n",
    "            last_loss = running_loss/1000\n",
    "            print('  batch {} loss: {}\\n'.format(i + 1, last_loss))\n",
    "          #  print(f\"Predictions look currently like: {outputs[:32]}\\n\\n\")\n",
    "            running_loss = 0.\n",
    "    return last_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f26fb8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( epochs=50):\n",
    "\n",
    "    best_vloss = 1_000_000.\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "     print('EPOCH {}:'.format(epoch + 1))\n",
    " \n",
    "   \n",
    "     model.train(True)\n",
    "     avg_loss = train_one_epoch(epoch)\n",
    "\n",
    "\n",
    "     running_vloss = 0.0\n",
    "    \n",
    "     model.eval()\n",
    "\n",
    "   \n",
    "     with torch.no_grad():\n",
    "        for i, vdata in enumerate(test_dataloader):\n",
    "            vchannel_tensor, vfrozen_tensor, vsnr_tensor, vtarget_tensor = vdata\n",
    "            voutputs = model(vchannel_tensor.float().to(device), vfrozen_tensor.int().to(device), vsnr_tensor.float().to(device))\n",
    "          #  B, L, C = voutputs.shape\n",
    "          #  vloss = loss_fn(voutputs.view(B*L, C).to(device), vlabels.view(B*L).to(device))\n",
    "            \n",
    "            vloss = calculate_loss(vfrozen_tensor.to(device), vtarget_tensor.to(device), voutputs.to(device))\n",
    "            running_vloss += vloss\n",
    "\n",
    "     avg_vloss = running_vloss / (i + 1)\n",
    "     print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    \n",
    "     scheduler.step(avg_vloss)\n",
    "\n",
    "    \n",
    "     if avg_vloss < best_vloss:\n",
    "      \n",
    "    \n",
    "      best_vloss = avg_vloss\n",
    "      model_path = f'./checkpoints/config_{CONFIG_NO}/model_epoch_{epoch}.pt'\n",
    "      torch.save({\n",
    "         \"comments\": \"Set the loss function to evaluate loss for both frozen and non frozen bits. set expand=3\",\n",
    "    'model_config': {\n",
    "        \"d_model\": model.d_model,\n",
    "        \"num_layer_encoder\": model.num_layer_encoder,\n",
    "        \"num_layers_bimamba_block\": model.num_layers_bimamba_block,\n",
    "        \"seq_len\": model.seq_len,\n",
    "        \"d_state\": model.d_state,\n",
    "        \"d_conv\": model.d_conv,\n",
    "        \"expand\": model.expand,\n",
    "    },\n",
    "    'epoch': epoch + 1,\n",
    "    'train_loss': avg_loss,\n",
    "    'val_loss': avg_vloss,\n",
    "    'state_dict': model.state_dict()\n",
    "}, model_path)\n",
    "\n",
    "\n",
    "     \n",
    "    print(\"Training completed. Model available to use\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d589f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 1000 loss: 0.21707879555225373\n",
      "\n",
      "  batch 2000 loss: 0.1917518671452999\n",
      "\n",
      "  batch 3000 loss: 0.18240208588540555\n",
      "\n",
      "  batch 4000 loss: 0.17807611653208733\n",
      "\n",
      "  batch 5000 loss: 0.17228572613745927\n",
      "\n",
      "  batch 6000 loss: 0.17099464400857686\n",
      "\n",
      "  batch 7000 loss: 0.16847753942012786\n",
      "\n",
      "  batch 8000 loss: 0.16549422273784875\n",
      "\n",
      "  batch 9000 loss: 0.16474470458179713\n",
      "\n",
      "  batch 10000 loss: 0.16465505230426788\n",
      "\n",
      "  batch 11000 loss: 0.1625800811946392\n",
      "\n",
      "  batch 12000 loss: 0.1610225873440504\n",
      "\n",
      "  batch 13000 loss: 0.16026050624996424\n",
      "\n",
      "  batch 14000 loss: 0.15957979682087897\n",
      "\n",
      "  batch 15000 loss: 0.15892285595089198\n",
      "\n",
      "  batch 16000 loss: 0.15895515418797732\n",
      "\n",
      "  batch 17000 loss: 0.15947539995610713\n",
      "\n",
      "  batch 18000 loss: 0.15894125133752823\n",
      "\n",
      "  batch 19000 loss: 0.1582379503250122\n",
      "\n",
      "  batch 20000 loss: 0.15855647423118352\n",
      "\n",
      "  batch 21000 loss: 0.15941906003654002\n",
      "\n",
      "  batch 22000 loss: 0.1581791153177619\n",
      "\n",
      "  batch 23000 loss: 0.1583668457120657\n",
      "\n",
      "  batch 24000 loss: 0.1573456007912755\n",
      "\n",
      "  batch 25000 loss: 0.1584183292761445\n",
      "\n",
      "  batch 26000 loss: 0.15859761864691974\n",
      "\n",
      "  batch 27000 loss: 0.158146659091115\n",
      "\n",
      "  batch 28000 loss: 0.1585507511124015\n",
      "\n",
      "  batch 29000 loss: 0.15853721028566362\n",
      "\n",
      "  batch 30000 loss: 0.1580207304880023\n",
      "\n",
      "  batch 31000 loss: 0.1581445607841015\n",
      "\n",
      "LOSS train 0.1581445607841015 valid 0.16026167571544647\n",
      "EPOCH 2:\n",
      "  batch 1000 loss: 0.15852053264528512\n",
      "\n",
      "  batch 2000 loss: 0.158074415281415\n",
      "\n",
      "  batch 3000 loss: 0.15786011421680451\n",
      "\n",
      "  batch 4000 loss: 0.15787307269126177\n",
      "\n",
      "  batch 5000 loss: 0.15774515829980373\n",
      "\n",
      "  batch 6000 loss: 0.1575214123427868\n",
      "\n",
      "  batch 7000 loss: 0.15960610608756542\n",
      "\n",
      "  batch 8000 loss: 0.1585308115184307\n",
      "\n",
      "  batch 9000 loss: 0.1582593119740486\n",
      "\n",
      "  batch 10000 loss: 0.15959767562150956\n",
      "\n",
      "  batch 11000 loss: 0.15762171275168657\n",
      "\n",
      "  batch 12000 loss: 0.1591718254685402\n",
      "\n",
      "  batch 13000 loss: 0.15920509777963163\n",
      "\n",
      "  batch 14000 loss: 0.15789536022394896\n",
      "\n",
      "  batch 15000 loss: 0.15848790681362152\n",
      "\n",
      "  batch 16000 loss: 0.1586122114509344\n",
      "\n",
      "  batch 17000 loss: 0.15895335542410613\n",
      "\n",
      "  batch 18000 loss: 0.15865127516537905\n",
      "\n",
      "  batch 19000 loss: 0.15851292656362057\n",
      "\n",
      "  batch 20000 loss: 0.15741814013570546\n",
      "\n",
      "  batch 21000 loss: 0.15823879660665988\n",
      "\n",
      "  batch 22000 loss: 0.1587163464948535\n",
      "\n",
      "  batch 23000 loss: 0.15945347044616937\n",
      "\n",
      "  batch 24000 loss: 0.15775587821006776\n",
      "\n",
      "  batch 25000 loss: 0.1584340114519\n",
      "\n",
      "  batch 26000 loss: 0.15932204912602901\n",
      "\n",
      "  batch 27000 loss: 0.15776901745051145\n",
      "\n",
      "  batch 28000 loss: 0.1577760246321559\n",
      "\n",
      "  batch 29000 loss: 0.15945181715488435\n",
      "\n",
      "  batch 30000 loss: 0.15904804522544144\n",
      "\n",
      "  batch 31000 loss: 0.15826351968944072\n",
      "\n",
      "LOSS train 0.15826351968944072 valid 0.15708871185779572\n",
      "EPOCH 3:\n",
      "  batch 1000 loss: 0.15856408461928367\n",
      "\n",
      "  batch 2000 loss: 0.15776449879258872\n",
      "\n",
      "  batch 3000 loss: 0.15840956941992043\n",
      "\n",
      "  batch 4000 loss: 0.15788921801745892\n",
      "\n",
      "  batch 5000 loss: 0.15814518616348505\n",
      "\n",
      "  batch 6000 loss: 0.15889498767256738\n",
      "\n",
      "  batch 7000 loss: 0.157520036585629\n",
      "\n",
      "  batch 8000 loss: 0.15937450955063104\n",
      "\n",
      "  batch 9000 loss: 0.15908264411985873\n",
      "\n",
      "  batch 10000 loss: 0.15672010504454376\n",
      "\n",
      "  batch 11000 loss: 0.15841233541816474\n",
      "\n",
      "  batch 12000 loss: 0.15848514013737441\n",
      "\n",
      "  batch 13000 loss: 0.15962993671000003\n",
      "\n",
      "  batch 14000 loss: 0.1595379918962717\n",
      "\n",
      "  batch 15000 loss: 0.1582816733121872\n",
      "\n",
      "  batch 16000 loss: 0.15792012689262627\n",
      "\n",
      "  batch 17000 loss: 0.1592737557440996\n",
      "\n",
      "  batch 18000 loss: 0.16019416300952435\n",
      "\n",
      "  batch 19000 loss: 0.15807110480964184\n",
      "\n",
      "  batch 20000 loss: 0.15754615617543458\n",
      "\n",
      "  batch 21000 loss: 0.1589663691520691\n",
      "\n",
      "  batch 22000 loss: 0.15861980152875185\n",
      "\n",
      "  batch 23000 loss: 0.15868226674199104\n",
      "\n",
      "  batch 24000 loss: 0.15944262132793666\n",
      "\n",
      "  batch 25000 loss: 0.15898408406227826\n",
      "\n",
      "  batch 26000 loss: 0.15800355602055788\n",
      "\n",
      "  batch 27000 loss: 0.15992781418561935\n",
      "\n",
      "  batch 28000 loss: 0.15845454736053943\n",
      "\n",
      "  batch 29000 loss: 0.1595086009427905\n",
      "\n",
      "  batch 30000 loss: 0.16115506843477487\n",
      "\n",
      "  batch 31000 loss: 0.15918268700689078\n",
      "\n",
      "LOSS train 0.15918268700689078 valid 0.15974409878253937\n",
      "EPOCH 4:\n",
      "  batch 1000 loss: 0.15847215512394905\n",
      "\n",
      "  batch 2000 loss: 0.1594943418651819\n",
      "\n",
      "  batch 3000 loss: 0.15975092279165984\n",
      "\n",
      "  batch 4000 loss: 0.15888189254701138\n",
      "\n",
      "  batch 5000 loss: 0.15814471045136452\n",
      "\n",
      "  batch 6000 loss: 0.1588000792041421\n",
      "\n",
      "  batch 7000 loss: 0.1587433901205659\n",
      "\n",
      "  batch 8000 loss: 0.15824826357513666\n",
      "\n",
      "  batch 9000 loss: 0.15945976739376783\n",
      "\n",
      "  batch 10000 loss: 0.15909250389784574\n",
      "\n",
      "  batch 11000 loss: 0.15819063691049814\n",
      "\n",
      "  batch 12000 loss: 0.15874413860589265\n",
      "\n",
      "  batch 13000 loss: 0.15793295598775148\n",
      "\n",
      "  batch 14000 loss: 0.15839954347908497\n",
      "\n",
      "  batch 15000 loss: 0.15813479491323232\n",
      "\n",
      "  batch 16000 loss: 0.1580986692607403\n",
      "\n",
      "  batch 17000 loss: 0.1588549068644643\n",
      "\n",
      "  batch 18000 loss: 0.15908198696374892\n",
      "\n",
      "  batch 19000 loss: 0.15876122599095105\n",
      "\n",
      "  batch 20000 loss: 0.1587216555327177\n",
      "\n",
      "  batch 21000 loss: 0.15800267765671014\n",
      "\n",
      "  batch 22000 loss: 0.1604397280961275\n",
      "\n",
      "  batch 23000 loss: 0.1580594069287181\n",
      "\n",
      "  batch 24000 loss: 0.15820393139868974\n",
      "\n",
      "  batch 25000 loss: 0.1588584755063057\n",
      "\n",
      "  batch 26000 loss: 0.15859180135279893\n",
      "\n",
      "  batch 27000 loss: 0.15871934953331948\n",
      "\n",
      "  batch 28000 loss: 0.1612739932090044\n",
      "\n",
      "  batch 29000 loss: 0.15886174827814104\n",
      "\n",
      "  batch 30000 loss: 0.15863715299218892\n",
      "\n",
      "  batch 31000 loss: 0.15987211138755084\n",
      "\n",
      "LOSS train 0.15987211138755084 valid 0.16047313809394836\n",
      "EPOCH 5:\n",
      "  batch 1000 loss: 0.1583453579992056\n",
      "\n",
      "  batch 2000 loss: 0.15812620899826288\n",
      "\n",
      "  batch 3000 loss: 0.15908412713557482\n",
      "\n",
      "  batch 4000 loss: 0.15876899223029614\n",
      "\n",
      "  batch 5000 loss: 0.15899340345710516\n",
      "\n",
      "  batch 6000 loss: 0.15978211531043052\n",
      "\n",
      "  batch 7000 loss: 0.15900127705931663\n",
      "\n",
      "  batch 8000 loss: 0.15868967925012112\n",
      "\n",
      "  batch 9000 loss: 0.15827948024123908\n",
      "\n",
      "  batch 10000 loss: 0.15855815357714892\n",
      "\n",
      "  batch 11000 loss: 0.15845239549130202\n",
      "\n",
      "  batch 12000 loss: 0.15936591853946447\n",
      "\n",
      "  batch 13000 loss: 0.15775016467273234\n",
      "\n",
      "  batch 14000 loss: 0.15891170097142457\n",
      "\n",
      "  batch 15000 loss: 0.15931817642599344\n",
      "\n",
      "  batch 16000 loss: 0.15905587375164032\n",
      "\n",
      "  batch 17000 loss: 0.1608821829557419\n",
      "\n",
      "  batch 18000 loss: 0.15734846916794776\n",
      "\n",
      "  batch 19000 loss: 0.15904767271876336\n",
      "\n",
      "  batch 20000 loss: 0.15850971861183644\n",
      "\n",
      "  batch 21000 loss: 0.1576096580773592\n",
      "\n",
      "  batch 22000 loss: 0.16070885071158408\n",
      "\n",
      "  batch 23000 loss: 0.15889708603918554\n",
      "\n",
      "  batch 24000 loss: 0.1586476144567132\n",
      "\n",
      "  batch 25000 loss: 0.15881987047195434\n",
      "\n",
      "  batch 26000 loss: 0.15946860618889333\n",
      "\n",
      "  batch 27000 loss: 0.15776491640508175\n",
      "\n",
      "  batch 28000 loss: 0.19599193278700114\n",
      "\n",
      "  batch 29000 loss: 0.1881672743856907\n",
      "\n",
      "  batch 30000 loss: 0.19262911680340766\n",
      "\n",
      "  batch 31000 loss: 0.1911758300215006\n",
      "\n",
      "LOSS train 0.1911758300215006 valid 0.18864651024341583\n",
      "EPOCH 6:\n",
      "  batch 1000 loss: 0.18959865695238112\n",
      "\n",
      "  batch 2000 loss: 0.29170909875631335\n",
      "\n",
      "  batch 3000 loss: 0.29436910520493986\n",
      "\n",
      "  batch 4000 loss: 0.29339176036417486\n",
      "\n",
      "  batch 5000 loss: 0.29124274030327796\n",
      "\n",
      "  batch 6000 loss: 0.2929546086490154\n",
      "\n",
      "  batch 7000 loss: 0.29245242787897585\n",
      "\n",
      "  batch 8000 loss: 0.2932160413563252\n",
      "\n",
      "  batch 9000 loss: 0.292502329275012\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(epochs)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mEPOCH \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[33m'\u001b[39m.format(epoch + \u001b[32m1\u001b[39m))\n\u001b[32m      9\u001b[39m model.train(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m avg_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m running_vloss = \u001b[32m0.0\u001b[39m\n\u001b[32m     15\u001b[39m model.eval()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(epoch_index)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m#    B, L, C = outputs.shape\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m#    output_logits = outputs.view(B*L, C).to(device)\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m#    target_flattened = shifted.view(B*L).to(device).long()\u001b[39;00m\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#    loss = loss_fn(output_logits, target_flattened)\u001b[39;00m\n\u001b[32m     23\u001b[39m     loss = calculate_loss(ip2, op, outputs)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m     28\u001b[39m     optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Code/mamba-seq-pred/venv/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Code/mamba-seq-pred/venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Code/mamba-seq-pred/venv/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train(epochs=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
